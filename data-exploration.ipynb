{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf31c2bb",
   "metadata": {},
   "source": [
    "# Full pipeline for Text Data Exploration\n",
    "\n",
    "As a data scientist specializing in Natural Language Processing (NLP), a thorough data exploration phase is crucial for understanding the text data, identifying patterns, and informing subsequent preprocessing and modeling steps. Here's a comprehensive pipeline with common tasks, tips, code, libraries, and useful charts, presented step-by-step in Python. The data used by this guide can be downloaded from https://zenodo.org/records/10157504."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17c875",
   "metadata": {},
   "source": [
    "# 1. Data Loading and Initial Inspection\n",
    "\n",
    "**Common Task**: Load your text data and get a first glance at its structure and content.\n",
    "\n",
    "**Tips**:\n",
    "- Start with a sample if your dataset is massive.\n",
    "- Understand the format: Is it a CSV, JSON, database, etc.?\n",
    "- Check for missing values immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3f7ff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\alesa ta\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   -------------------------------- ------- 4/5 [nltk]\n",
      "   ---------------------------------------- 5/5 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 joblib-1.5.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9a3b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d84ad61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Alesa\n",
      "[nltk_data]     TA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Alesa\n",
      "[nltk_data]     TA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Alesa\n",
      "[nltk_data]     TA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5315ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_input = '*.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a312d02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AllProductReviews.csv', 'data-exploration.ipynb', 'README.md']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understanding the type of file, looking for the extension file\n",
    "file_list = glob.glob(file_input)\n",
    "len(file_list)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews =pd.read_csv(file_list[0], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f18ecb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewTitle</th>\n",
       "      <th>ReviewBody</th>\n",
       "      <th>ReviewStar</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14332</th>\n",
       "      <td>Good\\n</td>\n",
       "      <td>Good\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14333</th>\n",
       "      <td>Amazing Product\\n</td>\n",
       "      <td>An amazing product but a bit costly.\\n</td>\n",
       "      <td>5</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14334</th>\n",
       "      <td>Not bad\\n</td>\n",
       "      <td>Sound\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14335</th>\n",
       "      <td>a good product\\n</td>\n",
       "      <td>the sound is good battery life is good but the...</td>\n",
       "      <td>5</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14336</th>\n",
       "      <td>Average headphones , n overrated name\\n</td>\n",
       "      <td>M writing this review after using for almost 7...</td>\n",
       "      <td>1</td>\n",
       "      <td>JBL T110BT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ReviewTitle  \\\n",
       "14332                                   Good\\n   \n",
       "14333                        Amazing Product\\n   \n",
       "14334                                Not bad\\n   \n",
       "14335                         a good product\\n   \n",
       "14336  Average headphones , n overrated name\\n   \n",
       "\n",
       "                                              ReviewBody  ReviewStar  \\\n",
       "14332                                             Good\\n           4   \n",
       "14333             An amazing product but a bit costly.\\n           5   \n",
       "14334                                            Sound\\n           1   \n",
       "14335  the sound is good battery life is good but the...           5   \n",
       "14336  M writing this review after using for almost 7...           1   \n",
       "\n",
       "          Product  \n",
       "14332  JBL T110BT  \n",
       "14333  JBL T110BT  \n",
       "14334  JBL T110BT  \n",
       "14335  JBL T110BT  \n",
       "14336  JBL T110BT  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31082e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14337 entries, 0 to 14336\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   ReviewTitle  14337 non-null  object\n",
      " 1   ReviewBody   14337 non-null  object\n",
      " 2   ReviewStar   14337 non-null  int64 \n",
      " 3   Product      14337 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 448.2+ KB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2518f7",
   "metadata": {},
   "source": [
    "There isn't null. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490eaa28",
   "metadata": {},
   "source": [
    "# 2. Basic Text Statistics\n",
    "\n",
    "**Common Tasks**: Calculate fundamental statistics about your text data to understand its overall characteristics.\n",
    "\n",
    "**Tips**:\n",
    "- Character count can indicate brevity or verbosity.\n",
    "- Word count and sentence count provide insights into text length and complexity.\n",
    "- Average word length can hint at the formality or simplicity of the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06d49a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data, replace \\n with \"\"\n",
    "reviews['ReviewTitle'] = reviews['ReviewTitle'].str.replace('\\n', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e715ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews['char_count_Title'] = reviews['ReviewTitle'].str.len()\n",
    "#all the analysis is made on body \n",
    "reviews['char_count'] = reviews['ReviewBody'].str.len()\n",
    "reviews['word_count'] = reviews['ReviewBody'].str.split().str.len()\n",
    "reviews['sentence_count'] = reviews['ReviewBody'].str.split('.').str.len()\n",
    "reviews['word_len'] = reviews['ReviewBody'].str.split().apply(lambda word_list: [len(word) for word in word_list])\n",
    "reviews['average_word_len'] = reviews['word_len'].apply(lambda counts: sum(counts)/len(counts) if counts else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e69cf780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewStar</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>average_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14337.000000</td>\n",
       "      <td>14337.000000</td>\n",
       "      <td>14337.000000</td>\n",
       "      <td>14337.000000</td>\n",
       "      <td>14337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.675874</td>\n",
       "      <td>126.584362</td>\n",
       "      <td>22.320709</td>\n",
       "      <td>3.666039</td>\n",
       "      <td>4.836041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.503409</td>\n",
       "      <td>154.807798</td>\n",
       "      <td>27.702611</td>\n",
       "      <td>3.910061</td>\n",
       "      <td>1.010389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5046.000000</td>\n",
       "      <td>864.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ReviewStar    char_count    word_count  sentence_count  \\\n",
       "count  14337.000000  14337.000000  14337.000000    14337.000000   \n",
       "mean       3.675874    126.584362     22.320709        3.666039   \n",
       "std        1.503409    154.807798     27.702611        3.910061   \n",
       "min        1.000000      1.000000      0.000000        1.000000   \n",
       "25%        3.000000     36.000000      6.000000        1.000000   \n",
       "50%        4.000000     88.000000     15.000000        2.000000   \n",
       "75%        5.000000    160.000000     28.000000        5.000000   \n",
       "max        5.000000   5046.000000    864.000000       65.000000   \n",
       "\n",
       "       average_word_len  \n",
       "count      14337.000000  \n",
       "mean           4.836041  \n",
       "std            1.010389  \n",
       "min            0.000000  \n",
       "25%            4.240000  \n",
       "50%            4.666667  \n",
       "75%            5.222222  \n",
       "max           31.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4460b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No doubt it has a great bass and to a great extent noise cancellation and decent sound clarity and mindblowing battery but the following dissapointed me though i tried a lot to adjust.1.Bluetooth range not more than 10m2. Pain in ear due the conical buds(can be removed)3. Wires are a bit long which makes it odd in front.4. No pouch provided.5. Worst part is very low quality and distoring mic. Other person keeps complaining about my voice.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()\n",
    "reviews['ReviewBody'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c630d27",
   "metadata": {},
   "source": [
    "# 3. Text Preprocessing (for Exploration)\n",
    "\n",
    "**Common Tasks**: Clean and normalize text to prepare it for frequency analysis and other exploratory tasks. This is a lighter preprocessing step compared to what you might do for modeling.\n",
    "\n",
    "**Tips**:\n",
    "- Lowercasing prevents treating \"The\" and \"the\" as different words.\n",
    "- Punctuation removal reduces noise.\n",
    "- Stopword removal focuses on meaningful content words.\n",
    "- Stemming/Lemmatization reduces words to their root forms, consolidating variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8615fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['ReviewBody'] = reviews['ReviewBody'].str.lower()\n",
    "reviews['ReviewBody'] = reviews['ReviewBody'].str.replace(rf\"[{string.punctuation}]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b783da2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Apply for each row\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m reviews[\u001b[33m'\u001b[39m\u001b[33mCleanedReview\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mreviews\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mReviewBody\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_stopwords\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mremove_stopwords\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m words = word_tokenize(text.lower())\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m filtered = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word.isalpha() \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43menglish\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(filtered)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alesa TA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\data.py:323\u001b[39m, in \u001b[36mFileSystemPathPointer.open\u001b[39m\u001b[34m(self, encoding)\u001b[39m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoding=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     stream = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    325\u001b[39m         stream = SeekableUnicodeStreamReader(stream, encoding)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Funtion to remove stopwords\n",
    "# def remove_stopwords(text):\n",
    "#     if not isinstance(text, str):  # evita errores si hay NaN u otros tipos\n",
    "#         return \"\"\n",
    "#     words = word_tokenize(text.lower())\n",
    "#     filtered = [word for word in words if word.isalpha() and word not in stopwords.words('english')]\n",
    "#     return \" \".join(filtered)\n",
    "# # Apply for each row\n",
    "# reviews['CleanedReview'] = reviews['ReviewBody'].apply(remove_stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8b163fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar recursos si no existen\n",
    "def ensure_nltk_resource(resource_name, resource_path):\n",
    "    try:\n",
    "        nltk.data.find(resource_path)\n",
    "    except LookupError:\n",
    "        nltk.download(resource_name)\n",
    "\n",
    "ensure_nltk_resource('punkt', 'tokenizers/punkt')\n",
    "ensure_nltk_resource('stopwords', 'corpora/stopwords')\n",
    "\n",
    "# Función de limpieza robusta\n",
    "def clean_text_remove_stopwords(text):\n",
    "    try:\n",
    "        # Verificar que sea texto\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Tokenizar\n",
    "        words = word_tokenize(text.lower())\n",
    "\n",
    "        # Filtrar: solo letras, sin stopwords\n",
    "        clean_words = [\n",
    "            word for word in words\n",
    "            if word.isalpha() and word not in stopwords.words('english')\n",
    "        ]\n",
    "\n",
    "        return \" \".join(clean_words)\n",
    "    except Exception as e:\n",
    "        # Si algo falla, devolver string vacío (y opcional: imprimir el error)\n",
    "        print(f\"Error al procesar: {text} → {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa6ae03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['CleanedReview'] = reviews['ReviewBody'].apply(clean_text_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632a2ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReviewBody\n",
      "<class 'str'>    14337\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(reviews['ReviewBody'].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "315db4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewTitle</th>\n",
       "      <th>ReviewBody</th>\n",
       "      <th>ReviewStar</th>\n",
       "      <th>Product</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_len</th>\n",
       "      <th>average_word_len</th>\n",
       "      <th>CleanedReview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honest review of an edm music lover</td>\n",
       "      <td>no doubt it has a great bass and to a great ex...</td>\n",
       "      <td>3</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "      <td>443</td>\n",
       "      <td>77</td>\n",
       "      <td>11</td>\n",
       "      <td>[2, 5, 2, 3, 1, 5, 4, 3, 2, 1, 5, 6, 5, 12, 3,...</td>\n",
       "      <td>4.753247</td>\n",
       "      <td>doubt great bass great extent noise cancellati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unreliable earphones with high cost</td>\n",
       "      <td>this  earphones are unreliable i bought it bef...</td>\n",
       "      <td>1</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "      <td>371</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>[4, 9, 3, 11, 1, 6, 2, 6, 2, 4, 9, 5, 4, 3, 4,...</td>\n",
       "      <td>4.781250</td>\n",
       "      <td>earphones unreliable bought days meanwhile rig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Really good and durable.</td>\n",
       "      <td>i bought itfor 999i purchased it second time g...</td>\n",
       "      <td>4</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "      <td>484</td>\n",
       "      <td>86</td>\n",
       "      <td>10</td>\n",
       "      <td>[1, 6, 5, 5, 9, 2, 6, 5, 6, 5, 3, 2, 8, 4, 2, ...</td>\n",
       "      <td>4.627907</td>\n",
       "      <td>bought itfor purchased second time gifted firs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped working in just 14 days</td>\n",
       "      <td>its sound quality is adorable overall it was g...</td>\n",
       "      <td>1</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "      <td>199</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, 5, 7, 2, 9, 7, 2, 3, 4, 3, 4, 3, 1, 5, 5, ...</td>\n",
       "      <td>4.378378</td>\n",
       "      <td>sound quality adorable overall good weeks stop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just Awesome Wireless Headphone under 1000...😉</td>\n",
       "      <td>its awesome good sound quality  89 hrs battery...</td>\n",
       "      <td>5</td>\n",
       "      <td>boAt Rockerz 255</td>\n",
       "      <td>235</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>[3, 10, 4, 5, 7, 1, 3, 3, 7, 7, 4, 4, 7, 1, 1,...</td>\n",
       "      <td>5.527778</td>\n",
       "      <td>awesome good sound quality hrs battery life wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ReviewTitle  \\\n",
       "0             Honest review of an edm music lover   \n",
       "1             Unreliable earphones with high cost   \n",
       "2                        Really good and durable.   \n",
       "3                 stopped working in just 14 days   \n",
       "4  Just Awesome Wireless Headphone under 1000...😉   \n",
       "\n",
       "                                          ReviewBody  ReviewStar  \\\n",
       "0  no doubt it has a great bass and to a great ex...           3   \n",
       "1  this  earphones are unreliable i bought it bef...           1   \n",
       "2  i bought itfor 999i purchased it second time g...           4   \n",
       "3  its sound quality is adorable overall it was g...           1   \n",
       "4  its awesome good sound quality  89 hrs battery...           5   \n",
       "\n",
       "            Product  char_count  word_count  sentence_count  \\\n",
       "0  boAt Rockerz 255         443          77              11   \n",
       "1  boAt Rockerz 255         371          64               4   \n",
       "2  boAt Rockerz 255         484          86              10   \n",
       "3  boAt Rockerz 255         199          37               4   \n",
       "4  boAt Rockerz 255         235          36              22   \n",
       "\n",
       "                                            word_len  average_word_len  \\\n",
       "0  [2, 5, 2, 3, 1, 5, 4, 3, 2, 1, 5, 6, 5, 12, 3,...          4.753247   \n",
       "1  [4, 9, 3, 11, 1, 6, 2, 6, 2, 4, 9, 5, 4, 3, 4,...          4.781250   \n",
       "2  [1, 6, 5, 5, 9, 2, 6, 5, 6, 5, 3, 2, 8, 4, 2, ...          4.627907   \n",
       "3  [3, 5, 7, 2, 9, 7, 2, 3, 4, 3, 4, 3, 1, 5, 5, ...          4.378378   \n",
       "4  [3, 10, 4, 5, 7, 1, 3, 3, 7, 7, 4, 4, 7, 1, 1,...          5.527778   \n",
       "\n",
       "                                       CleanedReview  \n",
       "0  doubt great bass great extent noise cancellati...  \n",
       "1  earphones unreliable bought days meanwhile rig...  \n",
       "2  bought itfor purchased second time gifted firs...  \n",
       "3  sound quality adorable overall good weeks stop...  \n",
       "4  awesome good sound quality hrs battery life wa...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c3128",
   "metadata": {},
   "source": [
    "# 4. Vocabulary Analysis\n",
    "\n",
    "**Common Tasks**: Understand the unique words, their frequencies, and patterns.\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "- Word clouds provide a quick visual summary of frequent terms.\n",
    "- Bar charts of top N words show exact frequencies.\n",
    "- Analyzing n-grams (bigrams, trigrams) reveals common phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25b608c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87ed15d4",
   "metadata": {},
   "source": [
    "# 5. Part-of-Speech (POS) Tagging\n",
    "\n",
    "**Common Task**: Analyze the distribution of grammatical categories (nouns, verbs, adjectives, etc.) in your text.\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "- Provides insights into the linguistic structure of your corpus.\n",
    "- Can highlight if your text is descriptive (many adjectives), action-oriented (many verbs), or topic-focused (many nouns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67be66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57be9595",
   "metadata": {},
   "source": [
    "# 6. Named Entity Recognition (NER)\n",
    "\n",
    "**Common Task**: Identify and categorize named entities (people, organizations, locations, dates, etc.) in your text.\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "- Reveals key subjects and concepts in your data.\n",
    "- Useful for extracting structured information from unstructured text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e5df89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bad472d2",
   "metadata": {},
   "source": [
    "# 7. Sentiment Analysis (if applicable)\n",
    "\n",
    "**Common Task**: Determine the emotional tone (positive, negative, neutral) of your text data.\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "- Provides a high-level understanding of the sentiment distribution.\n",
    "- Can be done with simple lexicon-based models or more complex pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa134e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdfbb4d3",
   "metadata": {},
   "source": [
    "# 8. Topic Modeling (High-level exploration)\n",
    "\n",
    "**Common Task**: Discover abstract \"topics\" that occur in a collection of documents.\n",
    "\n",
    "**Tips**:\n",
    "\n",
    "- LDA (Latent Dirichlet Allocation) is a common algorithm.\n",
    "- Requires a document-term matrix.\n",
    "- Provides a sense of the main themes present in your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e200749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
